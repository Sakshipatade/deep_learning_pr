# Step A: Import Required Libraries
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import cifar10
import matplotlib.pyplot as plt
import numpy as np

# Step B: Load Pretrained CNN (Transfer Learning Base Model)
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze all layers (so their weights don't change during training)
for layer in base_model.layers:
    layer.trainable = False

print("✅ Base model loaded and layers frozen.")

# Step C: Add Custom Classifier (Our Trainable Layers)
x = base_model.output
x = GlobalAveragePooling2D()(x)      # Converts feature maps to vector
x = Dense(256, activation='relu')(x) # Fully connected layer
output = Dense(10, activation='softmax')(x)  # CIFAR-10 has 10 classes

model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Step D: Load and Preprocess CIFAR-10 Dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Resize CIFAR-10 images from (32,32) to (224,224) to fit MobileNetV2
x_train = tf.image.resize(x_train, (224, 224))
x_test = tf.image.resize(x_test, (224, 224))

# Normalize pixel values (0–1)
x_train = x_train / 255.0
x_test = x_test / 255.0

# Convert labels to one-hot encoding
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Optional Data Augmentation
train_datagen = ImageDataGenerator(
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)
test_datagen = ImageDataGenerator()

train_data = train_datagen.flow(x_train, y_train, batch_size=32)
test_data = test_datagen.flow(x_test, y_test, batch_size=32)

print("✅ CIFAR-10 data loaded and preprocessed successfully!")

# Step E: Train Only the Custom Layers
history = model.fit(train_data, validation_data=test_data, epochs=5)

# Step F: Fine-tuning (Optional)
# Unfreeze last 20 layers for fine-tuning
for layer in base_model.layers[-20:]:
    layer.trainable = True

model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Fine-tune model for 3 epochs
history_fine = model.fit(train_data, validation_data=test_data, epochs=3)

# Step G: Plot Accuracy and Loss
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc (Initial)')
plt.plot(history.history['val_accuracy'], label='Val Acc (Initial)')
plt.plot(history_fine.history['accuracy'], label='Train Acc (Fine-tuned)')
plt.plot(history_fine.history['val_accuracy'], label='Val Acc (Fine-tuned)')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss (Initial)')
plt.plot(history.history['val_loss'], label='Val Loss (Initial)')
plt.plot(history_fine.history['loss'], label='Train Loss (Fine-tuned)')
plt.plot(history_fine.history['val_loss'], label='Val Loss (Fine-tuned)')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
